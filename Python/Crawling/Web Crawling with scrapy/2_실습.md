# 웹 크롤링을 이용한 서비스 개발

### 1. Scrapy 를 이용해 클리앙, 보배드림 크롤링

##### 0) 준비

- 가상머신 접속


- 가상환경 접속

  ```bash
  $ workon myenv
  ```

- scrapy project 생성

  ```bash
  $ scrapy startproject community
  ```

  ​

##### 1) clien : 모두의 공원

- https://www.clien.net/service/board/park
- 제목, 글쓴이, url, 날짜 조회수



###### items.py

```python
import scrapy


class CommunityItem(scrapy.Item):
    # define the fields for your item here like:
    # name = scrapy.Field()

    source = scrapy.Field()
    category = scrapy.Field()
    title = scrapy.Field()
    url = scrapy.Field()
    hits = scrapy.Field()
    date = scrapy.Field()

    pass
```



###### spiders 생성 : communitySpider.py

```python
# -*- coding: utf-8 -*-

__author__ = 'jjuya'

import scrapy,re

from community.items import CommunityItem
from datetime import datetime

class CommunitySpider(scrapy.Spider):
    name = "communityCrawler"

    #start_urls = []
    def start_requests(self):
        for i in range(1, 2, 1):
            yield scrapy.Request("https://www.clien.net/service/board/park?&od=T31&po=%d" % i, self.parse_clien)

    
```

- `yield scrapy.Request(..)`
  - `scrapy.Request`를 하나씩 쌓겠다.
- `self.parse_clien`
  - 실질적인 데이터 분석할 부분



###### parser 생성 : communitySpider.py 

```python
# ....

	def parse_clien(self, response):
        for sel in response.xpath('//*[@id="div_content"]/div[@class="list_item symph_row"]'):
            item = CommunityItem()

            item['source'] = 'clien'
            item['category'] = 'free'

            title = sel.xpath('div[@class="list_title"]/a[@class="list_subject"]/span/text()').extract()[0] # .extract_first()
            item['title'] = title.encode('utf-8')

            url = 'https://www.clien.net' + sel.xpath('div[@class="list_title"]/a/@href').extract()[0]
            item['url'] = url

            dateTmp = datetime.strptime(sel.xpath('div[@class="list_time"]/span[@class="time popover"]/span[@class="timestamp"]/text()').extract()[0], "%Y-%m-%d %H:%M:%S")
            item['date'] = dateTmp.strftime("%Y-%m-%d %H:%M:%S")

            hit = sel.xpath('div[@class="list_hit"]/span[@class="hit"]/text()').extract()[0]
            item['hits'] = hit

            print '=' * 50
            print item['title']

            yield item

```



##### 2) 보배드림 : 자유게시판 

- http://www.bobaedream.co.kr/list?code=freeb



###### start_requests 정의 : communitySpider.py

```python
	def start_requests(self):
        for i in range(1, 2, 1):
            yield scrapy.Request("https://www.clien.net/service/board/park?&od=T31&po=%d" % i, self.parse_clien)
            yield scrapy.Request("http://www.bobaedream.co.kr/list?code=freeb&page=" % i, self.parse_bobae)

```

- 크롤링은 주기적으로 이루어지는 거니까 주기 고민해서 최신 어디까지 가저올건지 고민.



###### parser 생성 : communitySpider.py

```python
	def parse_bobae(self, response):
        for sel in response.xpath('//tbody/tr[@itemtype="http://schema.org/Article"]'):
            item = CommunityItem()

            item['source'] = 'bobae'
            item['category'] = 'free'

            title = sel.xpath('//td[@class="pl14"]/a[@class="bsubject"]/text()').extract()[0]
            item['title'] = title.encode('utf-8')

            url = 'http://www.bobaedream.co.kr' + sel.xpath('//td[@class="pl14"]/a[@class="bsubject"]/@href').extract()[0]
            item['url'] = url

            date_now = datetime.now()
            date_str_tmp = sel.xpath('//td[@class="date"]/text()').extract()[0]

            prog = re.compile('[0-9]{2}:[0-9]{2}')
            if prog.match(date_str_tmp):
                date_str = date_now.strftime('%y/%m/%d') + ' ' + date_str_tmp + ':00'
            else:
                date_str = date_now.strftime('%y/') + date_str_tmp + ' ' + '00:00:00'

            dateTmp = datetime.strptime(date_str, "%Y-%m-%d %H:%M:%S")
            item['date'] = dateTmp.strftime("%Y-%m-%d %H:%M:%S")

            hit = int(sel.xpath('td[@class="count"]/text()').extract()[0])
            item['hits'] = hit

            print '=' * 50
            print item['title']

            yield item
```

- `prog = re.compile('[0-9]{2}:[0-9]{2}')` : 정규표현식



##### 3) pipeline

###### 특정 단어 필터링 : pipelines.py

```python
# -*- coding: utf-8 -*-

# Define your item pipelines here
#
# Don't forget to add your pipeline to the ITEM_PIPELINES setting
# See: https://doc.scrapy.org/en/latest/topics/item-pipeline.html

from scrapy.exceptions import DropItem

class CommunityPipeline(object):
    words_to_filter = [u'19금', u'후방주의']

    # 처리할 로직
    def process_item(self, item, spider):
        for word in words_to_filter:
            if word in unicode(item['title']):
                raise DropItem("Contains forbidden word: %s" % word)
            else:
                return item
```

- `DropItem("Contains forbidden word: %s" % word)`
  - `item`을 들어오지 못하게 막는다.
  - `exception`을 통해 발생



###### pipeline 등록 : settings.py

```python
ITEM_PIPELINES = {
   'community.pipelines.CommunityPipeline': 300,
}
```



###### log file 정의 : settings.py 

```python
import logging

# ...

LOG_FILE = 'logfile.log'
LOG_LEVEL = logging.INFO
```

